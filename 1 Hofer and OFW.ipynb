{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Importing libraries and module and some setting for notebook\n",
    "\n",
    "import pandas as pd \n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct  #Cosine Similarity\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data - Hofer\n",
    "df_hofer=pd.read_csv(\"products_hofer.csv\")\n",
    "df_hofer=df_hofer[['Hofer_Number','Product Name (Produktvariante)','brands/producer']]\n",
    "\n",
    "# Reading Data - OpenFoodWorld\n",
    "df_ofw=pd.read_csv(\"0 Preliminary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Name and GTIN null values\n",
    "df_ofw=df_ofw.dropna(subset=['Name','GTIN'])\n",
    "# Dropping GTIN duplicates\n",
    "df_ofw=df_ofw.drop_duplicates(subset=['GTIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting index\n",
    "df_hofer['Index']='HF '+df_hofer['Hofer_Number'].astype(str)\n",
    "df_ofw['Index'] = 'OFW '+df_ofw.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting new df with brand + name\n",
    "df_hofer['Product']=df_hofer['brands/producer'].fillna('')+' '+df_hofer['Product Name (Produktvariante)']\n",
    "df_ofw['Product']=df_ofw['Brand'].fillna('')+' '+df_ofw['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete GTIN starting with 0 \n",
    "df_ofw=df_ofw[df_ofw['GTIN'].astype(str).str[0] != '0']\n",
    "# Delete Products with less than 4 characters\n",
    "df_ofw=df_ofw[df_ofw['Product'].str.len()>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a concat df\n",
    "frames=[df_hofer,df_ofw]\n",
    "df=pd.concat(frames)\n",
    "df = df[[\"Index\", \"Product\",\"GTIN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1373285\n",
      "[0, 457762, 915524, 1373285]\n",
      "[457762, 457762, 457761]\n"
     ]
    }
   ],
   "source": [
    "# PRELIMINARY STAGE OF MEMORY REDUCTION\n",
    "\n",
    "# Get length of df_ofw\n",
    "length=len(df_ofw)\n",
    "\n",
    "# Get range of df_ofw to be splited into 6\n",
    "num, div = length, 3\n",
    "arr=[num // div + (1 if x < num % div else 0)  for x in range (div)]\n",
    "lang=[]\n",
    "\n",
    "# array of 6 splited ranges\n",
    "for i in range(len(arr)+1):\n",
    "    lang.append(sum(arr[:i]))\n",
    "\n",
    "print(length)\n",
    "print(lang)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458834"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ofw_temp1=df_ofw[:457762]\n",
    "temp1=[df_ofw_temp1,df_hofer]\n",
    "df1=pd.concat(temp1)\n",
    "len(df1)\n",
    "# df1.head(-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1072"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_hofer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457762"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ofw_temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ngrams(here we are taking n = 3 thus 3-gram (trigrams ) as  most room types only contain two or three words\n",
    "#  used for cleaning and removing some punctuation (dots, comma’s etc) i.e.((,-./)) from a string \n",
    "#  and generate and collect all n-grams of the string.  \n",
    "\n",
    " \n",
    "def ngrams(string, n=3):\n",
    "\n",
    "#     string = re.sub(r' \\d+%.',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "# # Testing ngrams work for verification \n",
    "# print(df['Product'].iloc[13214])\n",
    "# ngrams(df['Product'].iloc[13214])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having each words split (token or  lemmas (n-gram generated items) ) into a vector and\n",
    "# Scikit-learn’s  Tfidfvectorizer aim to do the same thing, which is to convert a collection of raw documents to a matrix of TF-IDF features. \n",
    "# Generate the matrix of TF-IDF (Term Frequency-Inverse Document frequency)values for each\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(df1['Product'].values.astype(str))\n",
    "\n",
    "# # View sparse CSR matrix.\n",
    "# print(tf_idf_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the similarity between two vectors of TF-IDF values the Cosine Similarity is usually used.\n",
    "# result matrix in a very sparse terms and Scikit-learn deals with this nicely by returning a sparse CSR matrix.\n",
    "\n",
    "def cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELFTIMED: 6943.999190568924\n"
     ]
    }
   ],
   "source": [
    "#  Run the optimized cosine similarity function. \n",
    "#  Only stores the top 10 most similar items with a similarity above 0.75\n",
    "\n",
    "t2 = time.time()\n",
    "matches = cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.73)\n",
    "t3 = time.time()-t2\n",
    "print(\"SELFTIMED:\", t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_matches_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2278634a5dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatches_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_matches_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmatches_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatches_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatches_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.99999\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# For removing all exact matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmatches_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatches_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# erasing repeating items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_matches_df' is not defined"
     ]
    }
   ],
   "source": [
    "matches_df = get_matches_df(matches, data.values.astype(str)) #data\n",
    "matches_df = matches_df[matches_df['similarity'] < 0.99999] # For removing all exact matches\n",
    "matches_df=matches_df.sort_values(['name'], ascending=False)\n",
    "\n",
    "# erasing repeating items\n",
    "matches_df = matches_df.drop_duplicates(subset=['similarity'],keep='first')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
